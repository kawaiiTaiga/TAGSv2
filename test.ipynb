{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation 과정 분산처리"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from Generation.generation_model import Generation_model\n",
    "from utils import *\n",
    "import asyncio\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM,LlamaTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "class Generation_model():\n",
    "    def __init__(self, device) -> None:\n",
    "        self.device = torch.device(f\"cuda:{device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model =  LlamaForCausalLM.from_pretrained(\"chavinlo/alpaca-native\",low_cpu_mem_usage=True,torch_dtype='auto').to(self.device)\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(\"chavinlo/alpaca-native\")\n",
    "        self.device_number = device\n",
    "    def generate(self,input_samples):\n",
    "        candidate_list = []\n",
    "        #placeholders  = \"'{}',\"* 1\n",
    "        #formatted_prompt = f\"prompt promp\" + placeholders + \"PROMPT PROMPT {}\".format(input_samples) \n",
    "        #print(input_samples)\n",
    "        #for input_sample in input_samples:\n",
    "            #prompt = formatted_prompt.format(*input_sample[1])\n",
    "        formatted_prompt = input_samples\n",
    "        inputs = self.tokenizer.encode(formatted_prompt,return_tensors=\"pt\").to(self.device)\n",
    "        candidate_senetnces = self.model.generate(inputs,max_new_tokens = 30) #make to candidate format\n",
    "        text = self.tokenizer.decode(*candidate_senetnces)\n",
    "       \n",
    "        return text\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4262cea059245c2bea88c4b6fa85782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from Generation.generation_model import Generation_model\n",
    "from utils import *\n",
    "import asyncio\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM,LlamaTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "model =  LlamaForCausalLM.from_pretrained(\"chavinlo/alpaca-native\",low_cpu_mem_usage=True,torch_dtype='auto').to(\"cuda:0\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"chavinlo/alpaca-native\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pubmed20k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Input sentences are part of a 'methods' in different medical papers. Please create five senetences in 'methods' part put new knowledge irrelevant with input sentences. Separate each sentence like 'sentence a:'\n",
      "\n",
      "### Input:\n",
      "'Pain was assessed using the visual analog pain scale','A total of 125 patients with primary knee OA were randomized 1:1'\n",
      "\n",
      "### Response:Sentence A: Pain was assessed using the Visual Analogue Scale (VAS). Sentence B: A total of 125 participants with Primary Knee Osteoarthritis (PKOA) were randomly assigned to either a treatment or control group. Sentence C: The VAS score ranged from 0-10 cm, where 0 represented no pain and 10 indicated severe pain. Sentence D: Patients completed questionnaires at baseline, three months and six months post intervention. Sentence E: Data analysis included descriptive statistics and inferential statistical tests as appropriate.</s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "instruction = \"Input sentences are part of a 'methods' in different medical papers. Please create five senetences in 'methods' part put new knowledge irrelevant with input sentences. Separate each sentence like 'sentence a:'\"\n",
    "input = \"\"\"'Pain was assessed using the visual analog pain scale','A total of 125 patients with primary knee OA were randomized 1:1'\"\"\"\n",
    "prompt = f\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "inputs = tokenizer.encode(prompt,return_tensors=\"pt\").to(\"cuda\")\n",
    "candidate_senetnces = model.generate(inputs,max_new_tokens = 200,repetition_penalty = 1.3) #make to candidate format\n",
    "text = tokenizer.decode(*candidate_senetnces)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'METHODS', 'score': 0.9533874988555908}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_tag = \"ml4pubmed/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext_pub_section\"\n",
    "classifier = pipeline(\n",
    "              'text-classification', \n",
    "              model=model_tag, \n",
    "            )\n",
    "            \n",
    "prompt = \"\"\"\n",
    "A total of 125 participants with Primary Knee Osteoarthritis (PKOA) were randomly assigned to either a treatment or control group.\n",
    "\"\"\"\n",
    "\n",
    "classifier(\n",
    "    prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "this is sentence of medical domain paper. Change sentence of given sentence into'mehtod' part sentence\n",
      "\n",
      "### Input:\n",
      "'Inflammation plays a major role in the pathogenesis of rheumatoid arthritis; anti inflammatory drugs such as steroids have long been used to treat this condition.'\n",
      "\n",
      "### Response:Methods \n",
      "• Inflammation plays a major role in the pathogenensis of rheumatoid arthritis.</s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "instruction = \"this is sentence of medical domain paper. Change sentence of given sentence into 'mehtod' part sentence\"\n",
    "input = \"\"\"'Inflammation plays a major role in the pathogenesis of rheumatoid arthritis; anti inflammatory drugs such as steroids have long been used to treat this condition.'\"\"\"\n",
    "prompt = f\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "inputs = tokenizer.encode(prompt,return_tensors=\"pt\").to(\"cuda\")\n",
    "candidate_senetnces = model.generate(inputs,max_new_tokens = 200,repetition_penalty = 1.35) #make to candidate format\n",
    "text = tokenizer.decode(*candidate_senetnces)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "input sentences are question about 'exchange rate', create five new question to ask about 'exchange rate',  Must be differenct with input setneces. Separate each sentence like 'sentence a:'\n",
      "\n",
      "### Input:\n",
      "'Do you have the best exchange rate?','What is the basis of your exchange rates?'\n",
      "\n",
      "### Response:Sentence A: Do you offer competitive exchange rates?\n",
      "Sentence B: How do you determine your exchange rates?\n",
      "Sentence C: What factors influence the fluctuation of exchange rates?\n",
      "Sentence D: Are there any additional fees charged for currency conversions?\n",
      "Sentence E: Is it possible to lock in an exchange rate?</s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "instruction = \"input sentences are question about 'exchange rate', create five new question to ask about 'exchange rate',  Must be differenct with input setneces. Separate each sentence like 'sentence a:'\"\n",
    "input = \"\"\"'Do you have the best exchange rate?','What is the basis of your exchange rates?'\"\"\"\n",
    "prompt = f\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "inputs = tokenizer.encode(prompt,return_tensors=\"pt\").to(\"cuda\")\n",
    "candidate_senetnces = model.generate(inputs,max_new_tokens = 200,repetition_penalty = 1.2) #make to candidate format\n",
    "text = tokenizer.decode(*candidate_senetnces)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"What is rhetorical feature in part of medical domain\"\n",
    "input = \"\"\"'Inflammation plays a major role in the pathogenesis of rheumatoid arthritis; anti inflammatory drugs such as steroids have long been used to treat this condition.'\"\"\"\n",
    "prompt = f\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "inputs = tokenizer.encode(prompt,return_tensors=\"pt\").to(\"cuda\")\n",
    "candidate_senetnces = model.generate(inputs,max_new_tokens = 200,repetition_penalty = 1.35) #make to candidate format\n",
    "text = tokenizer.decode(*candidate_senetnces)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/nlp-04/.cache/huggingface/datasets/ml4pubmed___csv/ml4pubmed--pubmed-classification-20k-3de662485795ef5d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e918fef9f0aa45d7ada99168c2695225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from Generation.generation_model import Generation_model\n",
    "from utils import *\n",
    "import asyncio\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM,LlamaTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ml4pubmed/pubmed-classification-20k\")\n",
    "original_data = preprocess_dataset(dataset['train'])\n",
    "few_dict = make_fewshot_dataset(original_data,2)\n",
    "few_data = list(few_dict.items())\n",
    "sublists_length = int(len(few_data)/2)\n",
    "sublists = [few_data[i:i+sublists_length] for i in range(0, len(few_data), sublists_length)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OBJECTIVE': ['This study aimed to examine : ( 1 ) the quality of motor patterns of children with DCD participating in AVG by comparing them with children with typical development ( TD ) and ( 2 ) whether differences existed in the motor patterns utilized with 2 AVG types : Sony PlayStation 3 Move and Microsoft Xbox 360 Kinect .', 'We compared three methods of phototherapy for the treatment of moderate to severe facial acne vulgaris in Chinese patients .'], 'METHODS': ['Preparation of Anethum Graveolens boiled solution was as follows : 10g ( two tablespoons ) of seed in 100cc water boiled for 10min .', 'Pain ratings were obtained using the Faces Pain Scale-Revised , and conversation during the procedure was evaluated using the Child-Adult Medical Procedure Interaction Scale-Revised by 2 independent observers .'], 'RESULTS': ['Vorapaxar increased GUSTO moderate/severe bleeding numerically in medically managed patients ( adjusted HR 1.46 , 95 % CI 0.99-2 .15 ) .', 'At the end of experiment , chickens treated with A. annua leaf powder had the highest body weight gain ( 68.2 g/day ) , after the negative control group , and the best feed conversion ( 1.85 ) among all experimental groups .'], 'CONCLUSIONS': ['Interventions focusing on controlling risk factors and prevention of worsening of neurological function may prevent poor HRQOL in these patients .', 'The efficacies of the novel CapStrepCis regimens were very similar .'], 'BACKGROUND': ['40 subjects with type 2 diabetes and moderate to severe CP were randomly distributed to groups receiving either NSPT or OHI .', 'To confirm the robustness of the primary analysis , PFS was also evaluated by blinded independent central review ( BICR ) .']}\n"
     ]
    }
   ],
   "source": [
    "print(few_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "@dataclass\n",
    "class generationArguments:\n",
    "    model_name: str = field(\n",
    "        default=\"chavinlo/alpaca-native\",\n",
    "        metadata = {\n",
    "            \"help\": \"The path for generation model\",\n",
    "        },\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chavinlo/alpaca-native'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = generationArguments()\n",
    "b.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.97s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.99s/it]\n"
     ]
    }
   ],
   "source": [
    "async def generation_with_model_1(Generation_model = None):\n",
    "    return Generation_model.generate(sublists[Generation_model.device_number])\n",
    "\n",
    "models = [Generation_model(device_number) for device_number in range(2)]\n",
    "async_list = [generation_with_model_1(model) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('OBJECTIVE', ['To assess the optimal scan delays and contrast injection durations for contrast-enhanced whole-body computed tomography ( CT ) .', 'The cross-sectional FOCUS ( Fixed-Dose Combination Drug for Secondary Cardiovascular Prevention ) study ( Phase 1 ) aimed to elucidate factors that interfere with appropriate adherence to CV medications for secondary prevention after an acute MI .']), ('METHODS', ['This large , multicenter , simulated-use study evaluated whether adults ( aged 18-65 years ) , caregivers ( parents/guardians aged 18-65 years of children aged 5-17 years ) , and children ( aged 11-17 years ) , with and without experience in using an EAI , had a preference for the current design of Auvi-Q or the current design of EpiPen .', 'Forty patients were treated with RF and 40 with Cryo .'])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/transformers/generation/utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('RESULTS', ['We analysed the rates of stroke and systemic embolism in 6563 aspirin-treated patients with AF from the ACTIVE-A/AVERROES databases .', 'LPV/r-based ART was superior to NVP-based ART for efficacy and safety outcomes ; however , those on NVP had larger improvements in CD4 % , weight and height z-scores .']), ('CONCLUSIONS', [\"OW individuals ' brain responses to food stimuli may indicate greater reward incentive processes than either SWL or NW groups .\", 'Extended use of an alcohol-based mouthrinse is no more likely to cause reduction in salivary flow or perceived dryness in individuals with normal salivary flow compared with a non-alcohol-based mouthrinse ( CPH ) .'])]\n"
     ]
    }
   ],
   "source": [
    "async def generation_with_model_1(Generation_model = None):\n",
    "    return Generation_model.generate(sublists[Generation_model.device_number])\n",
    "    \n",
    "    await asyncio.sleep(0)\n",
    "async_list = [generation_with_model_1(model) for model in models]\n",
    "async def main():\n",
    "    results = await asyncio.gather(\n",
    "        *async_list\n",
    "    )\n",
    "    return results\n",
    "    \n",
    "\n",
    "\n",
    "result = await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['OBJECTIVE', ' prompt promp\\'{}\\',PROMPT PROMPT [(\\'OBJECTIVE\\', [\\'To assess the optimal scan delays and contrast injection durations for contrast-enhanced whole-body computed tomography ( CT ).\\', \\'The cross-sectional FOCUS ( Fixed-Dose Combination Drug for Secondary Cardiovascular Prevention ) study ( Phase 1 ) aimed to elucidate factors that interfere with appropriate adherence to CV medications for secondary prevention after an acute MI.\\']), (\\'METHODS\\', [\\'This large, multicenter, simulated-use study evaluated whether adults ( aged 18-65 years ), caregivers ( parents/guardians aged 18-65 years of children aged 5-17 years ), and children ( aged 11-17 years ), with and without experience in using an EAI, had a preference for the current design of Auvi-Q or the current design of EpiPen.\\', \\'Forty patients were treated with RF and 40 with Cryo.\\'])]\\n\\ndef extract_paragraphs(paragraphs):\\n    paragraphs_list = paragraphs.split(\"\\\\n\\\\n\")\\n    return'], ['METHODS', ' prompt promp\\'{}\\',PROMPT PROMPT [(\\'OBJECTIVE\\', [\\'To assess the optimal scan delays and contrast injection durations for contrast-enhanced whole-body computed tomography ( CT ).\\', \\'The cross-sectional FOCUS ( Fixed-Dose Combination Drug for Secondary Cardiovascular Prevention ) study ( Phase 1 ) aimed to elucidate factors that interfere with appropriate adherence to CV medications for secondary prevention after an acute MI.\\']), (\\'METHODS\\', [\\'This large, multicenter, simulated-use study evaluated whether adults ( aged 18-65 years ), caregivers ( parents/guardians aged 18-65 years of children aged 5-17 years ), and children ( aged 11-17 years ), with and without experience in using an EAI, had a preference for the current design of Auvi-Q or the current design of EpiPen.\\', \\'Forty patients were treated with RF and 40 with Cryo.\\'])]\\n\\ndef extract_paragraphs(paragraphs):\\n    paragraphs_list = paragraphs.split(\"\\\\n\\\\n\")\\n    return']], [['RESULTS', ' prompt promp\\'{}\\',PROMPT PROMPT [(\\'RESULTS\\', [\\'We analysed the rates of stroke and systemic embolism in 6563 aspirin-treated patients with AF from the ACTIVE-A/AVERROES databases.\\', \\'LPV/r-based ART was superior to NVP-based ART for efficacy and safety outcomes ; however, those on NVP had larger improvements in CD4 %, weight and height z-scores.\\']), (\\'CONCLUSIONS\\', [\"OW individuals\\'brain responses to food stimuli may indicate greater reward incentive processes than either SWL or NW groups.\", \\'Extended use of an alcohol-based mouthrinse is no more likely to cause reduction in salivary flow or perceived dryness in individuals with normal salivary flow compared with a non-alcohol-based mouthrinse ( CPH ).\\'])]\\n\\ndef extract_paragraphs(text):\\n    text_lst = text.split()\\n    return text_lst\\n\\ndef extract'], ['CONCLUSIONS', ' prompt promp\\'{}\\',PROMPT PROMPT [(\\'RESULTS\\', [\\'We analysed the rates of stroke and systemic embolism in 6563 aspirin-treated patients with AF from the ACTIVE-A/AVERROES databases.\\', \\'LPV/r-based ART was superior to NVP-based ART for efficacy and safety outcomes ; however, those on NVP had larger improvements in CD4 %, weight and height z-scores.\\']), (\\'CONCLUSIONS\\', [\"OW individuals\\'brain responses to food stimuli may indicate greater reward incentive processes than either SWL or NW groups.\", \\'Extended use of an alcohol-based mouthrinse is no more likely to cause reduction in salivary flow or perceived dryness in individuals with normal salivary flow compared with a non-alcohol-based mouthrinse ( CPH ).\\'])]\\n\\ndef extract_paragraphs(text):\\n    text_lst = text.split()\\n    return text_lst\\n\\ndef extract']]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The median time to prostate-specific antigen progression was 4.7 months ( 95 % confidence interval 3.7-8 .3 ) ; the median overall survival was 11.8 months .', 'There was a weak correlation between prior contraceptive compliance and education level .']\n",
      "prompt promp'The median time to prostate-specific antigen progression was 4.7 months ( 95 % confidence interval 3.7-8 .3 ) ; the median overall survival was 11.8 months .',PROMPT PROMPT There was a weak correlation between prior contraceptive compliance and education level .\n"
     ]
    }
   ],
   "source": [
    "d = ['The median time to prostate-specific antigen progression was 4.7 months ( 95 % confidence interval 3.7-8 .3 ) ; the median overall survival was 11.8 months .', 'There was a weak correlation between prior contraceptive compliance and education level .']\n",
    "print(d)\n",
    "placeholders  = \"'{}',\"* 1\n",
    "formatted_prompt = \"prompt promp\" + placeholders + \"PROMPT PROMPT {}\" \n",
    "prompt = formatted_prompt.format(*d)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(Generation_model\u001b[39m.\u001b[39mgenerate(\u001b[39m\"\u001b[39m\u001b[39mhello\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m async_list \u001b[39m=\u001b[39m [generation_with_model_1(model) \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models]\n\u001b[0;32m----> 4\u001b[0m b\u001b[39m=\u001b[39m  generation_with_model_1(model1)\n\u001b[1;32m      5\u001b[0m c \u001b[39m=\u001b[39m  generation_with_model_1(model2)\n\u001b[1;32m      6\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "async def generation_with_model_1(Generation_model = None):\n",
    "    print(Generation_model.generate(\"hello\"))\n",
    "async_list = [generation_with_model_1(model) for model in models]\n",
    "b=  generation_with_model_1(model1)\n",
    "c =  generation_with_model_1(model2)\n",
    "async def main():\n",
    "    await asyncio.gather(\n",
    "        b,c\n",
    "    )\n",
    "    print(\"finsh\")\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "An asyncio.Future, a coroutine or an awaitable is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mawait\u001b[39;00m main()\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[0;32m---> 19\u001b[0m     \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39;49mgather(\n\u001b[1;32m     20\u001b[0m         \u001b[39m*\u001b[39;49masync_list\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     22\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfinsh\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda/envs/kkm/lib/python3.9/asyncio/tasks.py:756\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(loop, return_exceptions, *coros_or_futures)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39mif\u001b[39;00m loop \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    752\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe loop argument is deprecated since Python 3.8, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    753\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mand scheduled for removal in Python 3.10.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    754\u001b[0m                   \u001b[39mDeprecationWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 756\u001b[0m \u001b[39mreturn\u001b[39;00m _gather(\u001b[39m*\u001b[39;49mcoros_or_futures, loop\u001b[39m=\u001b[39;49mloop, return_exceptions\u001b[39m=\u001b[39;49mreturn_exceptions)\n",
      "File \u001b[0;32m~/anaconda/envs/kkm/lib/python3.9/asyncio/tasks.py:829\u001b[0m, in \u001b[0;36m_gather\u001b[0;34m(loop, return_exceptions, *coros_or_futures)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m coros_or_futures:\n\u001b[1;32m    828\u001b[0m     \u001b[39mif\u001b[39;00m arg \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m arg_to_fut:\n\u001b[0;32m--> 829\u001b[0m         fut \u001b[39m=\u001b[39m ensure_future(arg, loop\u001b[39m=\u001b[39;49mloop)\n\u001b[1;32m    830\u001b[0m         \u001b[39mif\u001b[39;00m loop \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    831\u001b[0m             loop \u001b[39m=\u001b[39m futures\u001b[39m.\u001b[39m_get_loop(fut)\n",
      "File \u001b[0;32m~/anaconda/envs/kkm/lib/python3.9/asyncio/tasks.py:677\u001b[0m, in \u001b[0;36mensure_future\u001b[0;34m(coro_or_future, loop)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[39mreturn\u001b[39;00m ensure_future(_wrap_awaitable(coro_or_future), loop\u001b[39m=\u001b[39mloop)\n\u001b[1;32m    676\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAn asyncio.Future, a coroutine or an awaitable is \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    678\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39mrequired\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: An asyncio.Future, a coroutine or an awaitable is required"
     ]
    }
   ],
   "source": [
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2558908/2895284244.py:3: RuntimeWarning: coroutine 'generation_with_model_1' was never awaited\n",
      "  globals()[f\"hell{a}\"] = generation_with_model_1(a)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "async_list = []\n",
    "for a in range(2):\n",
    "    globals()[f\"hell{a}\"] = generation_with_model_1(a)\n",
    "    async_list.append(f\"hell{a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hell'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset csv (/home/nlp-04/.cache/huggingface/datasets/ml4pubmed___csv/ml4pubmed--pubmed-classification-20k-3de662485795ef5d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 3/3 [00:00<00:00, 469.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ml4pubmed/pubmed-classification-20k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'###24854809'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# 리스트 순회하며 라벨에 따라 요소를 분류\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m dataset[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m---> 14\u001b[0m     label_dict[item[\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m]]\u001b[39m.\u001b[39mappend(item[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     16\u001b[0m \u001b[39m# 결과 출력\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m label, values \u001b[39min\u001b[39;00m label_dict\u001b[39m.\u001b[39mitems():\n",
      "\u001b[0;31mKeyError\u001b[0m: '###24854809'"
     ]
    }
   ],
   "source": [
    "labels = set()\n",
    "\n",
    "# 유니크한 라벨 추출\n",
    "for item in dataset['train']:\n",
    "    label = item['label']\n",
    "    labels.add(label)\n",
    "\n",
    "labels = ['RESULTS','METHODS','OBJECTIVE','CONCLUSIONS']\n",
    "# 라벨에 따라 요소를 분류할 딕셔너리 초기화\n",
    "label_dict = {label: [] for label in labels}\n",
    "\n",
    "# 리스트 순회하며 라벨에 따라 요소를 분류\n",
    "for item in dataset['train']:\n",
    "    label_dict[item['label']].append(item['text'])\n",
    "\n",
    "# 결과 출력\n",
    "for label, values in label_dict.items():\n",
    "    print(label + ':', values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = list(set(get_labels(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/nlp-04/.cache/huggingface/datasets/ml4pubmed___csv/ml4pubmed--pubmed-classification-20k-3de662485795ef5d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 3/3 [00:00<00:00, 584.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ml4pubmed/pubmed-classification-20k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset):\n",
    "# 레이블 정보 가져오기\n",
    "    labels = dataset['label']  # 훈련 데이터셋의 레이블 가져오기\n",
    "\n",
    "    # 레이블에 해당하는 인덱스 식별하기\n",
    "    label_indices = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        if label not in label_indices:\n",
    "            label_indices[label] = []\n",
    "        label_indices[label].append(i)\n",
    "\n",
    "    # 레이블별로 데이터 저장할 리스트 생성\n",
    "    label_lists = {}\n",
    "    for label, indices in label_indices.items():\n",
    "        if len(indices) > 1:  # 데이터가 있는 경우에만 리스트 생성\n",
    "            label_lists[label] = []\n",
    "\n",
    "    # 레이블별로 데이터 저장\n",
    "    for label, indices in label_indices.items():\n",
    "        if label in label_lists:  # 데이터가 있는 레이블에 대해서만 처리\n",
    "            for index in indices:\n",
    "                data = dataset[index]  # 데이터 가져오기 (여기서는 예시로 훈련 데이터셋 사용)\n",
    "                label_lists[label].append(data['text'])\n",
    "    return label_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = preprocess_dataset(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def make_fewshot_dataset(label_lists,num_samples):\n",
    "    fewshot_data = {}\n",
    "    for label,all_data in label_lists.items():\n",
    "        few_data = random.sample(all_data,num_samples)\n",
    "        fewshot_data[label] = few_data\n",
    "    return fewshot_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('OBJECTIVE',\n",
       "   ['To evaluate the efficacy of a combined neurolytic block of the celiac and superior hypogastric plexuses for incapacitating upper abdominal cancer pain .',\n",
       "    'To investigate the role of preoperative biometry for selecting initial contact lens power .']),\n",
       "  ('METHODS',\n",
       "   ['Children aged 9-12years from both sexes were randomly assigned to one of six groups to receive plain milk , fortified milk , plain orange juice , fortified orange juice , supplement or placebo .',\n",
       "    'Fasting blood samples were taken at baseline and after an 8-week intervention for quantification of related factors .']),\n",
       "  ('RESULTS',\n",
       "   ['No significant differences were found in the expression of sarcoplasmic reticulum Ca ( 2 + ) ATPase ( SERCA2a ) and ryanodine receptors ( RyRs ) .',\n",
       "    'In an analysis controlling for level , degree , and number of anchors , tranexamic acid reduced drain output and total blood losses .']),\n",
       "  ('CONCLUSIONS',\n",
       "   ['However , there is a need for further research to be considered as an alternative to chlorhexidine for prevention of VALP in ICU patients .',\n",
       "    '( Funded by a consortium of eight device and drug manufacturers and others ; DAPT ClinicalTrials.gov number , NCT00977938 . )'])],\n",
       " [('BACKGROUND',\n",
       "   ['Current treatments for low back pain have small effects .',\n",
       "    'Acupuncture is commonly used to reduce pain during labour despite contradictory results .'])]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_data = list(make_fewshot_dataset(b,2).items())\n",
    "sublists_length = 4 #class length / device lenght\n",
    "sublists = [few_data[i:i+sublists_length] for i in range(0, len(few_data), sublists_length)]\n",
    "sublists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForCL: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForCL from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForCL from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForCL were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['mlp.dense.weight', 'mlp.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /data/1_data_server/kkm/TAGSv2/data/csv/default-8d2b31c95c183785/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 3968.12it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 272.84it/s]\n",
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /data/1_data_server/kkm/TAGSv2/data/csv/default-8d2b31c95c183785/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.27it/s]\n",
      "                                                                      \r"
     ]
    }
   ],
   "source": [
    "from Selection import Train_Selection_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "07/03/2023 11:55:20 - INFO - Selection.simcse.trainers -   ***** Running training *****\n",
      "07/03/2023 11:55:20 - INFO - Selection.simcse.trainers -     Num examples = 275601\n",
      "07/03/2023 11:55:20 - INFO - Selection.simcse.trainers -     Num Epochs = 3\n",
      "07/03/2023 11:55:20 - INFO - Selection.simcse.trainers -     Instantaneous batch size per device = 8\n",
      "07/03/2023 11:55:20 - INFO - Selection.simcse.trainers -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "07/03/2023 11:55:20 - INFO - Selection.simcse.trainers -     Gradient Accumulation steps = 1\n",
      "07/03/2023 11:55:20 - INFO - Selection.simcse.trainers -     Total optimization steps = 103353\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='52' max='103353' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    52/103353 00:02 < 1:30:59, 18.92 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Train_Selection_model\u001b[39m.\u001b[39;49mtrain_selection_model()\n",
      "File \u001b[0;32m/data/1_data_server/kkm/TAGSv2/Selection/Train_Selection_model.py:354\u001b[0m, in \u001b[0;36mtrain_selection_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_selection_model\u001b[39m():\n\u001b[0;32m--> 354\u001b[0m     train_result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain(model_path \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m/data/1_data_server/kkm/TAGSv2/Selection/simcse/trainers.py:497\u001b[0m, in \u001b[0;36mCLTrainer.train\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m    496\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    499\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    501\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/kkm/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/kkm/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/kkm/lib/python3.10/site-packages/transformers/optimization.py:446\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[39m# In-place operations to update the averages at the same time\u001b[39;00m\n\u001b[1;32m    445\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m(\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta1))\n\u001b[0;32m--> 446\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39;49maddcmul_(grad, grad, value\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m \u001b[39m-\u001b[39;49m beta2)\n\u001b[1;32m    447\u001b[0m denom \u001b[39m=\u001b[39m exp_avg_sq\u001b[39m.\u001b[39msqrt()\u001b[39m.\u001b[39madd_(group[\u001b[39m\"\u001b[39m\u001b[39meps\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    449\u001b[0m step_size \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Train_Selection_model.train_selection_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/data/1_data_server/kkm/TAGSv2/data/csv/default-8d2b31c95c183785/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4202b1a26026490099010534c18a8478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45bfdada425c4aca95408e2a1d8814b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7668acba98b84891822492a29fe9fd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728ea0f0ab98431c98ef29fe90a8bcc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extension = 'csv'\n",
    "datasets = load_dataset(extension, data_files='/data/1_data_server/kkm/TAGSv2/SimCSE/data/nli_for_simcse.csv', cache_dir=\"./data/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sent0', 'sent1', 'hard_neg']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Selection import Train_Selection_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForCL: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForCL from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForCL from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForCL were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['mlp.dense.weight', 'mlp.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "selection = Train_Selection_model.selection_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/04/2023 14:40:10 - WARNING - datasets.builder -   Found cached dataset csv (/data/1_data_server/kkm/TAGSv2/data/csv/default-8d2b31c95c183785/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c827dc852f48ea82fee4ee612ff375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29da6aa0665488fbffbca25e62b338c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/275601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp-04/anaconda/envs/kkm/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "07/04/2023 14:40:38 - INFO - Selection.simcse.trainers -   ***** Running training *****\n",
      "07/04/2023 14:40:38 - INFO - Selection.simcse.trainers -     Num examples = 275601\n",
      "07/04/2023 14:40:38 - INFO - Selection.simcse.trainers -     Num Epochs = 3\n",
      "07/04/2023 14:40:38 - INFO - Selection.simcse.trainers -     Instantaneous batch size per device = 8\n",
      "07/04/2023 14:40:38 - INFO - Selection.simcse.trainers -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "07/04/2023 14:40:38 - INFO - Selection.simcse.trainers -     Gradient Accumulation steps = 1\n",
      "07/04/2023 14:40:38 - INFO - Selection.simcse.trainers -     Total optimization steps = 103353\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='103353' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     3/103353 00:00 < 1:34:49, 18.17 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m selection\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/data/1_data_server/kkm/TAGSv2/Selection/Train_Selection_model.py:448\u001b[0m, in \u001b[0;36mselection_model.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m trainer\u001b[39m.\u001b[39mmodel_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_args\n\u001b[1;32m    447\u001b[0m trainer\u001b[39m.\u001b[39muse_amp \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m train_result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/data/1_data_server/kkm/TAGSv2/Selection/simcse/trainers.py:464\u001b[0m, in \u001b[0;36mCLTrainer.train\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    462\u001b[0m         tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m    463\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m    465\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_total_flos \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfloating_point_ops(inputs)\n\u001b[1;32m    467\u001b[0m \u001b[39mif\u001b[39;00m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m (\n\u001b[1;32m    468\u001b[0m     \u001b[39m# last step in epoch but step is always smaller than gradient_accumulation_steps\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     steps_in_epoch \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n\u001b[1;32m    470\u001b[0m     \u001b[39mand\u001b[39;00m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m steps_in_epoch\n\u001b[1;32m    471\u001b[0m ):\n\u001b[1;32m    472\u001b[0m     \u001b[39m# Gradient clipping\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/envs/kkm/lib/python3.9/site-packages/transformers/trainer.py:2717\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2715\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[1;32m   2716\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2717\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m   2719\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/anaconda/envs/kkm/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda/envs/kkm/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "selection.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def create_contrastive_data(file_path, data_dict):\n",
    "    with open(file_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile,delimiter='\\t')\n",
    "        writer.writerow(['Anchor', 'Positive', 'Negative'])\n",
    "\n",
    "        for label, data_list in data_dict.items():\n",
    "            for i, anchor in enumerate(data_list):\n",
    "                positive_samples = data_list[:i] + data_list[i+1:]\n",
    "\n",
    "                for negative_label, negative_data_list in data_dict.items():\n",
    "                    if negative_label != label:\n",
    "                        for negative in negative_data_list:\n",
    "                            writer.writerow([anchor, positive_samples[0], negative])\n",
    "\n",
    "create_contrastive_data('contrastive_data.tsv',few_dict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /data/1_data_server/kkm/TAGSv2/data/csv/default-b78a69759be7cf11/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 2057.04it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 644.09it/s]\n",
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /data/1_data_server/kkm/TAGSv2/data/csv/default-b78a69759be7cf11/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 331.88it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "datasets =  load_dataset('csv',delimiter='\\t',data_files='/data/1_data_server/kkm/TAGSv2/contrastive_data.tsv', cache_dir=\"./data/\")"
   ]
  },
  {
=======
>>>>>>> 876da8f37c8ff7a3acb9549bab255cf615f07039
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Generation_model():\n",
    "    def __init__(self, device) -> None:\n",
    "        self.device = torch.device(f\"cuda:{device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model =  LlamaForCausalLM.from_pretrained(\"chavinlo/alpaca-native\",low_cpu_mem_usage=True,torch_dtype='auto').to(self.device)\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(\"chavinlo/alpaca-native\")\n",
    "        self.device_number = device\n",
    "    def generate(self,input_samples):\n",
    "        candidate_list = []\n",
    "        placeholders  = \"'{}',\"* 1\n",
    "        formatted_prompt = f\"prompt promp\" + placeholders + \"PROMPT PROMPT {}\".format(input_samples) \n",
    "        print(input_samples)\n",
    "        for input_sample in input_samples:\n",
    "            prompt = formatted_prompt.format(*input_sample[1])\n",
    "            inputs = self.tokenizer.encode(formatted_prompt,return_tensors=\"pt\").to(self.device)\n",
    "            candidate_senetnces = self.model.generate(inputs) #make to candidate format\n",
    "            text = self.tokenizer.decode(*candidate_senetnces)\n",
    "            candidate_list.append([input_sample[0],text])\n",
    "\n",
    "        return candidate_list\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset csv (/home/nlp-04/.cache/huggingface/datasets/ml4pubmed___csv/ml4pubmed--pubmed-classification-20k-3de662485795ef5d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 3/3 [00:00<00:00, 446.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from Generation.generation_model import Generation_model\n",
    "from utils import *\n",
    "import asyncio\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM,LlamaTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ml4pubmed/pubmed-classification-20k\")\n",
    "original_data = preprocess_dataset(dataset['train'])\n",
    "few_data = list(make_fewshot_dataset(original_data,2).items())\n",
    "sublists_length = int(len(few_data)/2)\n",
    "sublists = [few_data[i:i+sublists_length] for i in range(0, len(few_data), sublists_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.97s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.99s/it]\n"
     ]
    }
   ],
   "source": [
    "async def generation_with_model_1(Generation_model = None):\n",
    "    return Generation_model.generate(sublists[Generation_model.device_number])\n",
    "\n",
    "models = [Generation_model(device_number) for device_number in range(2)]\n",
    "async_list = [generation_with_model_1(model) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('OBJECTIVE', ['To assess the optimal scan delays and contrast injection durations for contrast-enhanced whole-body computed tomography ( CT ) .', 'The cross-sectional FOCUS ( Fixed-Dose Combination Drug for Secondary Cardiovascular Prevention ) study ( Phase 1 ) aimed to elucidate factors that interfere with appropriate adherence to CV medications for secondary prevention after an acute MI .']), ('METHODS', ['This large , multicenter , simulated-use study evaluated whether adults ( aged 18-65 years ) , caregivers ( parents/guardians aged 18-65 years of children aged 5-17 years ) , and children ( aged 11-17 years ) , with and without experience in using an EAI , had a preference for the current design of Auvi-Q or the current design of EpiPen .', 'Forty patients were treated with RF and 40 with Cryo .'])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/transformers/generation/utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('RESULTS', ['We analysed the rates of stroke and systemic embolism in 6563 aspirin-treated patients with AF from the ACTIVE-A/AVERROES databases .', 'LPV/r-based ART was superior to NVP-based ART for efficacy and safety outcomes ; however , those on NVP had larger improvements in CD4 % , weight and height z-scores .']), ('CONCLUSIONS', [\"OW individuals ' brain responses to food stimuli may indicate greater reward incentive processes than either SWL or NW groups .\", 'Extended use of an alcohol-based mouthrinse is no more likely to cause reduction in salivary flow or perceived dryness in individuals with normal salivary flow compared with a non-alcohol-based mouthrinse ( CPH ) .'])]\n"
     ]
    }
   ],
   "source": [
    "async def generation_with_model_1(Generation_model = None):\n",
    "    return Generation_model.generate(sublists[Generation_model.device_number])\n",
    "    \n",
    "    await asyncio.sleep(0)\n",
    "async_list = [generation_with_model_1(model) for model in models]\n",
    "async def main():\n",
    "    results = await asyncio.gather(\n",
    "        *async_list\n",
    "    )\n",
    "    return results\n",
    "    \n",
    "\n",
    "\n",
    "result = await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['OBJECTIVE', ' prompt promp\\'{}\\',PROMPT PROMPT [(\\'OBJECTIVE\\', [\\'To assess the optimal scan delays and contrast injection durations for contrast-enhanced whole-body computed tomography ( CT ).\\', \\'The cross-sectional FOCUS ( Fixed-Dose Combination Drug for Secondary Cardiovascular Prevention ) study ( Phase 1 ) aimed to elucidate factors that interfere with appropriate adherence to CV medications for secondary prevention after an acute MI.\\']), (\\'METHODS\\', [\\'This large, multicenter, simulated-use study evaluated whether adults ( aged 18-65 years ), caregivers ( parents/guardians aged 18-65 years of children aged 5-17 years ), and children ( aged 11-17 years ), with and without experience in using an EAI, had a preference for the current design of Auvi-Q or the current design of EpiPen.\\', \\'Forty patients were treated with RF and 40 with Cryo.\\'])]\\n\\ndef extract_paragraphs(paragraphs):\\n    paragraphs_list = paragraphs.split(\"\\\\n\\\\n\")\\n    return'], ['METHODS', ' prompt promp\\'{}\\',PROMPT PROMPT [(\\'OBJECTIVE\\', [\\'To assess the optimal scan delays and contrast injection durations for contrast-enhanced whole-body computed tomography ( CT ).\\', \\'The cross-sectional FOCUS ( Fixed-Dose Combination Drug for Secondary Cardiovascular Prevention ) study ( Phase 1 ) aimed to elucidate factors that interfere with appropriate adherence to CV medications for secondary prevention after an acute MI.\\']), (\\'METHODS\\', [\\'This large, multicenter, simulated-use study evaluated whether adults ( aged 18-65 years ), caregivers ( parents/guardians aged 18-65 years of children aged 5-17 years ), and children ( aged 11-17 years ), with and without experience in using an EAI, had a preference for the current design of Auvi-Q or the current design of EpiPen.\\', \\'Forty patients were treated with RF and 40 with Cryo.\\'])]\\n\\ndef extract_paragraphs(paragraphs):\\n    paragraphs_list = paragraphs.split(\"\\\\n\\\\n\")\\n    return']], [['RESULTS', ' prompt promp\\'{}\\',PROMPT PROMPT [(\\'RESULTS\\', [\\'We analysed the rates of stroke and systemic embolism in 6563 aspirin-treated patients with AF from the ACTIVE-A/AVERROES databases.\\', \\'LPV/r-based ART was superior to NVP-based ART for efficacy and safety outcomes ; however, those on NVP had larger improvements in CD4 %, weight and height z-scores.\\']), (\\'CONCLUSIONS\\', [\"OW individuals\\'brain responses to food stimuli may indicate greater reward incentive processes than either SWL or NW groups.\", \\'Extended use of an alcohol-based mouthrinse is no more likely to cause reduction in salivary flow or perceived dryness in individuals with normal salivary flow compared with a non-alcohol-based mouthrinse ( CPH ).\\'])]\\n\\ndef extract_paragraphs(text):\\n    text_lst = text.split()\\n    return text_lst\\n\\ndef extract'], ['CONCLUSIONS', ' prompt promp\\'{}\\',PROMPT PROMPT [(\\'RESULTS\\', [\\'We analysed the rates of stroke and systemic embolism in 6563 aspirin-treated patients with AF from the ACTIVE-A/AVERROES databases.\\', \\'LPV/r-based ART was superior to NVP-based ART for efficacy and safety outcomes ; however, those on NVP had larger improvements in CD4 %, weight and height z-scores.\\']), (\\'CONCLUSIONS\\', [\"OW individuals\\'brain responses to food stimuli may indicate greater reward incentive processes than either SWL or NW groups.\", \\'Extended use of an alcohol-based mouthrinse is no more likely to cause reduction in salivary flow or perceived dryness in individuals with normal salivary flow compared with a non-alcohol-based mouthrinse ( CPH ).\\'])]\\n\\ndef extract_paragraphs(text):\\n    text_lst = text.split()\\n    return text_lst\\n\\ndef extract']]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The median time to prostate-specific antigen progression was 4.7 months ( 95 % confidence interval 3.7-8 .3 ) ; the median overall survival was 11.8 months .', 'There was a weak correlation between prior contraceptive compliance and education level .']\n",
      "prompt promp'The median time to prostate-specific antigen progression was 4.7 months ( 95 % confidence interval 3.7-8 .3 ) ; the median overall survival was 11.8 months .',PROMPT PROMPT There was a weak correlation between prior contraceptive compliance and education level .\n"
     ]
    }
   ],
   "source": [
    "d = ['The median time to prostate-specific antigen progression was 4.7 months ( 95 % confidence interval 3.7-8 .3 ) ; the median overall survival was 11.8 months .', 'There was a weak correlation between prior contraceptive compliance and education level .']\n",
    "print(d)\n",
    "placeholders  = \"'{}',\"* 1\n",
    "formatted_prompt = \"prompt promp\" + placeholders + \"PROMPT PROMPT {}\" \n",
    "prompt = formatted_prompt.format(*d)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(Generation_model\u001b[39m.\u001b[39mgenerate(\u001b[39m\"\u001b[39m\u001b[39mhello\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m async_list \u001b[39m=\u001b[39m [generation_with_model_1(model) \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models]\n\u001b[0;32m----> 4\u001b[0m b\u001b[39m=\u001b[39m  generation_with_model_1(model1)\n\u001b[1;32m      5\u001b[0m c \u001b[39m=\u001b[39m  generation_with_model_1(model2)\n\u001b[1;32m      6\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "async def generation_with_model_1(Generation_model = None):\n",
    "    print(Generation_model.generate(\"hello\"))\n",
    "async_list = [generation_with_model_1(model) for model in models]\n",
    "b=  generation_with_model_1(model1)\n",
    "c =  generation_with_model_1(model2)\n",
    "async def main():\n",
    "    await asyncio.gather(\n",
    "        b,c\n",
    "    )\n",
    "    print(\"finsh\")\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "An asyncio.Future, a coroutine or an awaitable is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mawait\u001b[39;00m main()\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[0;32m---> 19\u001b[0m     \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39;49mgather(\n\u001b[1;32m     20\u001b[0m         \u001b[39m*\u001b[39;49masync_list\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     22\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfinsh\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda/envs/kkm/lib/python3.9/asyncio/tasks.py:756\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(loop, return_exceptions, *coros_or_futures)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39mif\u001b[39;00m loop \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    752\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe loop argument is deprecated since Python 3.8, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    753\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mand scheduled for removal in Python 3.10.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    754\u001b[0m                   \u001b[39mDeprecationWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 756\u001b[0m \u001b[39mreturn\u001b[39;00m _gather(\u001b[39m*\u001b[39;49mcoros_or_futures, loop\u001b[39m=\u001b[39;49mloop, return_exceptions\u001b[39m=\u001b[39;49mreturn_exceptions)\n",
      "File \u001b[0;32m~/anaconda/envs/kkm/lib/python3.9/asyncio/tasks.py:829\u001b[0m, in \u001b[0;36m_gather\u001b[0;34m(loop, return_exceptions, *coros_or_futures)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m coros_or_futures:\n\u001b[1;32m    828\u001b[0m     \u001b[39mif\u001b[39;00m arg \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m arg_to_fut:\n\u001b[0;32m--> 829\u001b[0m         fut \u001b[39m=\u001b[39m ensure_future(arg, loop\u001b[39m=\u001b[39;49mloop)\n\u001b[1;32m    830\u001b[0m         \u001b[39mif\u001b[39;00m loop \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    831\u001b[0m             loop \u001b[39m=\u001b[39m futures\u001b[39m.\u001b[39m_get_loop(fut)\n",
      "File \u001b[0;32m~/anaconda/envs/kkm/lib/python3.9/asyncio/tasks.py:677\u001b[0m, in \u001b[0;36mensure_future\u001b[0;34m(coro_or_future, loop)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[39mreturn\u001b[39;00m ensure_future(_wrap_awaitable(coro_or_future), loop\u001b[39m=\u001b[39mloop)\n\u001b[1;32m    676\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAn asyncio.Future, a coroutine or an awaitable is \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    678\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39mrequired\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: An asyncio.Future, a coroutine or an awaitable is required"
     ]
    }
   ],
   "source": [
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2558908/2895284244.py:3: RuntimeWarning: coroutine 'generation_with_model_1' was never awaited\n",
      "  globals()[f\"hell{a}\"] = generation_with_model_1(a)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "async_list = []\n",
    "for a in range(2):\n",
    "    globals()[f\"hell{a}\"] = generation_with_model_1(a)\n",
    "    async_list.append(f\"hell{a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hell'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp-04/anaconda3/envs/kkm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset csv (/home/nlp-04/.cache/huggingface/datasets/ml4pubmed___csv/ml4pubmed--pubmed-classification-20k-3de662485795ef5d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 3/3 [00:00<00:00, 469.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ml4pubmed/pubmed-classification-20k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'###24854809'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# 리스트 순회하며 라벨에 따라 요소를 분류\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m dataset[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m---> 14\u001b[0m     label_dict[item[\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m]]\u001b[39m.\u001b[39mappend(item[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     16\u001b[0m \u001b[39m# 결과 출력\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m label, values \u001b[39min\u001b[39;00m label_dict\u001b[39m.\u001b[39mitems():\n",
      "\u001b[0;31mKeyError\u001b[0m: '###24854809'"
     ]
    }
   ],
   "source": [
    "labels = set()\n",
    "\n",
    "# 유니크한 라벨 추출\n",
    "for item in dataset['train']:\n",
    "    label = item['label']\n",
    "    labels.add(label)\n",
    "\n",
    "labels = ['RESULTS','METHODS','OBJECTIVE','CONCLUSIONS']\n",
    "# 라벨에 따라 요소를 분류할 딕셔너리 초기화\n",
    "label_dict = {label: [] for label in labels}\n",
    "\n",
    "# 리스트 순회하며 라벨에 따라 요소를 분류\n",
    "for item in dataset['train']:\n",
    "    label_dict[item['label']].append(item['text'])\n",
    "\n",
    "# 결과 출력\n",
    "for label, values in label_dict.items():\n",
    "    print(label + ':', values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = list(set(get_labels(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/nlp-04/.cache/huggingface/datasets/ml4pubmed___csv/ml4pubmed--pubmed-classification-20k-3de662485795ef5d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 3/3 [00:00<00:00, 584.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ml4pubmed/pubmed-classification-20k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset):\n",
    "# 레이블 정보 가져오기\n",
    "    labels = dataset['label']  # 훈련 데이터셋의 레이블 가져오기\n",
    "\n",
    "    # 레이블에 해당하는 인덱스 식별하기\n",
    "    label_indices = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        if label not in label_indices:\n",
    "            label_indices[label] = []\n",
    "        label_indices[label].append(i)\n",
    "\n",
    "    # 레이블별로 데이터 저장할 리스트 생성\n",
    "    label_lists = {}\n",
    "    for label, indices in label_indices.items():\n",
    "        if len(indices) > 1:  # 데이터가 있는 경우에만 리스트 생성\n",
    "            label_lists[label] = []\n",
    "\n",
    "    # 레이블별로 데이터 저장\n",
    "    for label, indices in label_indices.items():\n",
    "        if label in label_lists:  # 데이터가 있는 레이블에 대해서만 처리\n",
    "            for index in indices:\n",
    "                data = dataset[index]  # 데이터 가져오기 (여기서는 예시로 훈련 데이터셋 사용)\n",
    "                label_lists[label].append(data['text'])\n",
    "    return label_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = preprocess_dataset(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def make_fewshot_dataset(label_lists,num_samples):\n",
    "    fewshot_data = {}\n",
    "    for label,all_data in label_lists.items():\n",
    "        few_data = random.sample(all_data,num_samples)\n",
    "        fewshot_data[label] = few_data\n",
    "    return fewshot_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('OBJECTIVE',\n",
       "   ['To evaluate the efficacy of a combined neurolytic block of the celiac and superior hypogastric plexuses for incapacitating upper abdominal cancer pain .',\n",
       "    'To investigate the role of preoperative biometry for selecting initial contact lens power .']),\n",
       "  ('METHODS',\n",
       "   ['Children aged 9-12years from both sexes were randomly assigned to one of six groups to receive plain milk , fortified milk , plain orange juice , fortified orange juice , supplement or placebo .',\n",
       "    'Fasting blood samples were taken at baseline and after an 8-week intervention for quantification of related factors .']),\n",
       "  ('RESULTS',\n",
       "   ['No significant differences were found in the expression of sarcoplasmic reticulum Ca ( 2 + ) ATPase ( SERCA2a ) and ryanodine receptors ( RyRs ) .',\n",
       "    'In an analysis controlling for level , degree , and number of anchors , tranexamic acid reduced drain output and total blood losses .']),\n",
       "  ('CONCLUSIONS',\n",
       "   ['However , there is a need for further research to be considered as an alternative to chlorhexidine for prevention of VALP in ICU patients .',\n",
       "    '( Funded by a consortium of eight device and drug manufacturers and others ; DAPT ClinicalTrials.gov number , NCT00977938 . )'])],\n",
       " [('BACKGROUND',\n",
       "   ['Current treatments for low back pain have small effects .',\n",
       "    'Acupuncture is commonly used to reduce pain during labour despite contradictory results .'])]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_data = list(make_fewshot_dataset(b,2).items())\n",
    "sublists_length = 4 #class length / device lenght\n",
    "sublists = [few_data[i:i+sublists_length] for i in range(0, len(few_data), sublists_length)]\n",
    "sublists"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kkm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.9.13"
=======
   "version": "3.10.9"
>>>>>>> 876da8f37c8ff7a3acb9549bab255cf615f07039
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
